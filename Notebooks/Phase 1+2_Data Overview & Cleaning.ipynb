{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d549ca-3644-4610-8862-272c6263fbba",
   "metadata": {},
   "source": [
    "# Global AI Job Market & Salary Trends 2025  \n",
    "## ðŸŽ¯ Project Report â€“ Targeted Analysis for SME Tech Recruiters\n",
    "\n",
    "---\n",
    "## Phase 1 â€“ Understand the question and identify analysis questions\n",
    "## 1.1. Project Context\n",
    "\n",
    "In a rapidly evolving technological landscape, small and medium-sized enterprises (SMEs) in the **technology sector** face increased challenges in attracting and retaining AI talent. With rising competition, evolving skill demands, and salary inflation driven by innovation waves, **HR teams must understand both market dynamics and salary benchmarks** to make informed recruitment decisions.\n",
    "\n",
    "This project is designed to **support SME tech recruiters** in optimizing their hiring strategy for AI professionals. Using a global dataset of over 15,000 AI-related job postings from 50+ countries, we provide market-level insights as well as a predictive salary model.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2. Project Objectives\n",
    "\n",
    "The project consists of two main parts:\n",
    "\n",
    "### 1.2.1 Descriptive Analysis\n",
    "- Identify which **industries and job roles** dominate the current AI job market.\n",
    "- Examine how **remote vs hybrid vs onsite work models** are evolving.\n",
    "- Explore trending roles with respect to **compensation**, **benefits**, and **required skills**.\n",
    "- Compare salary distributions and trends to understand market volatility.\n",
    "\n",
    "### 1.2.2 Predictive Modeling\n",
    "- Build a **salary prediction model** using variables such as skills, company size, job location, education, experience level, year, and job type.\n",
    "- Quantify the **most influential factors** affecting compensation and explain their underlying patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3. Analytical Questions\n",
    "\n",
    "This study seeks to answer the following key questions:\n",
    "\n",
    "1. **Which AI-related job titles are in highest demand across different quarters and months?**\n",
    "2. **Which job roles benefit or suffer from AI disruption**, as seen through salary and demand metrics?\n",
    "3. **What are the most essential skills** across industries and job roles, and how are they correlated with salary and experience?\n",
    "4. **Which features most strongly correlate with salary**, including role, experience, skills, industry, and location?\n",
    "5. **Are salary distributions within the same industry significantly different across job titles**, and what explains those differences?\n",
    "\n",
    "---\n",
    "\n",
    "## 1.4. Target Audience\n",
    "\n",
    "This report is written for:\n",
    "- **Hiring managers and recruiters** in small-to-medium tech companies\n",
    "- **Workforce planners** seeking to adapt to AI-related labor trends\n",
    "- **Data enthusiasts** and **junior analysts** looking to explore real-world HR analytics\n",
    "- **Career developers** aiming to understand which AI skills offer the most market value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee9fd1-393c-4800-9907-3127990696bc",
   "metadata": {},
   "source": [
    "## Phase 2. Data Overview\n",
    "\n",
    "This section provides an initial inspection of the dataset structure and key variables. The dataset contains over 15,000 AI job listings collected globally, each represented as a row with information about job title, salary, required skills, company characteristics, and metadata related to posting time and remote work ratio.\n",
    "\n",
    "### Preview of the First 5 Records\n",
    "We begin by displaying the first five rows to get an idea of the data format and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e000d5-8d8c-4fdc-b6db-4b04f8ff88a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    job_id              job_title  salary_usd salary_currency  \\\n",
      "0  AI00001  ai research scientist     90376.0             USD   \n",
      "1  AI00002   AI Software Engineer         NaN             USD   \n",
      "2  AI00003          AI SPECIALIST    152626.0             USD   \n",
      "3  AI00004           NLP Enginner     80215.0             USD   \n",
      "4  AI00005          AI Consultant     54624.0             EUR   \n",
      "\n",
      "  experience_level employment_type company_location company_size  \\\n",
      "0               SE              CT            China            M   \n",
      "1               EN              CT           Canada            M   \n",
      "2               MI              FL      Switzerland            L   \n",
      "3               SE              FL            India            M   \n",
      "4               EN              PT           France            S   \n",
      "\n",
      "  employee_residence  remote_ratio  \\\n",
      "0              China            50   \n",
      "1            Ireland           100   \n",
      "2        South Korea             0   \n",
      "3              India            50   \n",
      "4          Singapore           100   \n",
      "\n",
      "                                   required_skills education_required  \\\n",
      "0         Tableau, PyTorch, Kubernetes, Linux, NLP           Bachelor   \n",
      "1  Deep Learning, AWS, Mathematics, Python, Docker             Master   \n",
      "2     Kubernetes, Deep Learning, Java, Hadoop, NLP          Associate   \n",
      "3                        Scala, SQL, Linux, Python                Phd   \n",
      "4                     MLOps, Java, Tableau, Python             Master   \n",
      "\n",
      "   years_experience    industry posting_date application_deadline  \\\n",
      "0                 9  Automotive   2024-10-18           2024-11-07   \n",
      "1                 1       Media   2024-11-20           2025-01-11   \n",
      "2                 2   Education   2025-03-18           2025-04-07   \n",
      "3                 7  Consulting   2024-12-23           2025-02-24   \n",
      "4                 0       Media   2025-04-15           2025-06-23   \n",
      "\n",
      "   job_description_length  benefits_score       company_name  \n",
      "0                    1076             5.9    Smart Analytics  \n",
      "1                    1268             5.2       TechCorp Inc  \n",
      "2                    1974             9.4    Autonomous Tech  \n",
      "3                    1345             8.6     Future Systems  \n",
      "4                    1989             6.6  Advanced Robotics  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load dataset and inspect the first 5 rows\n",
    "# Purpose: To review column names, data types, and spot potential irregularities (e.g., missing values, formatting issues)\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Read Dataset\n",
    "df = pd.read_csv(\"C:/Users/theha/OneDrive/Desktop/Projects/Global AI Job Market & Salary Trends 2025/Dataset/raw_data_ai_job_dataset.csv\")\n",
    "\n",
    "# Screening of data structure\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb1f43-bb56-49f1-ac1a-0f42d944f7c6",
   "metadata": {},
   "source": [
    "### Initial Observations\n",
    "\n",
    "- The dataset includes a rich set of variables, such as:\n",
    "  - `job_title`, `required_skills`, `education_required`, and `years_experience` for job qualifications\n",
    "  - `salary_usd`, `salary_currency`, `company_size`, and `remote_ratio` for compensation and work structure\n",
    "  - `company_location` and `employee_residence` for geographic scope\n",
    "  - Time-based features such as `posting_date` and `application_deadline`\n",
    "\n",
    "- Some salary values are missing (e.g., row 2), which may require imputation or exclusion.\n",
    "\n",
    "- The `required_skills` column contains comma-separated values, which will later be transformed for skill-level analysis.\n",
    "\n",
    "- Categorical columns are encoded in abbreviated forms:\n",
    "  - `experience_level`: EN = Entry, MI = Mid, SE = Senior, EX = Executive\n",
    "  - `employment_type`: FT, PT, CT, FL\n",
    "  - `company_size`: S = Small, M = Medium, L = Large\n",
    "\n",
    "These observations will guide the cleaning and transformation steps that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246abdc7-9cb0-4564-9acb-a6901bb9ee4f",
   "metadata": {},
   "source": [
    "### 2.1 Data Structure and Missing Values\n",
    "\n",
    "To better understand the dataset structure and data quality, we examine the column types, non-null counts, and memory usage.\n",
    "\n",
    "The dataset contains **15,150 rows** and **19 columns**, representing various aspects of AI-related job listings across different companies and regions. Below is a breakdown:\n",
    "\n",
    "- **Numerical columns**: `salary_usd`, `salary_local`, `years_experience`, `job_description_length`, `benefits_score`, `remote_ratio`\n",
    "- **Categorical/textual columns**: `job_title`, `company_location`, `experience_level`, `education_required`, `industry`, `employment_type`, etc.\n",
    "- **Date columns**: `posting_date`, `application_deadline` (to be converted to datetime format)\n",
    "\n",
    "### Missing Data Overview\n",
    "\n",
    "Several columns contain missing values:\n",
    "- `salary_usd`: 762 missing values\n",
    "- `required_skills`: 755 missing\n",
    "- `education_required`: 537 missing\n",
    "\n",
    "These missing values will be addressed using appropriate imputation techniques or filtered out, depending on their role in the modeling or analysis phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc45c0d-669e-4ee8-916b-52329a9b6513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15150 entries, 0 to 15149\n",
      "Data columns (total 19 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   job_id                  15150 non-null  object \n",
      " 1   job_title               15150 non-null  object \n",
      " 2   salary_usd              14398 non-null  float64\n",
      " 3   salary_currency         15150 non-null  object \n",
      " 4   experience_level        15150 non-null  object \n",
      " 5   employment_type         15150 non-null  object \n",
      " 6   company_location        15150 non-null  object \n",
      " 7   company_size            15150 non-null  object \n",
      " 8   employee_residence      15150 non-null  object \n",
      " 9   remote_ratio            15150 non-null  int64  \n",
      " 10  required_skills         14395 non-null  object \n",
      " 11  education_required      14613 non-null  object \n",
      " 12  years_experience        15150 non-null  int64  \n",
      " 13  industry                15150 non-null  object \n",
      " 14  posting_date            15150 non-null  object \n",
      " 15  application_deadline    15150 non-null  object \n",
      " 16  job_description_length  15150 non-null  int64  \n",
      " 17  benefits_score          15150 non-null  float64\n",
      " 18  company_name            15150 non-null  object \n",
      "dtypes: float64(2), int64(3), object(14)\n",
      "memory usage: 2.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "required_skills           755\n",
       "salary_usd                752\n",
       "education_required        537\n",
       "job_title                   0\n",
       "job_id                      0\n",
       "employment_type             0\n",
       "salary_currency             0\n",
       "company_location            0\n",
       "company_size                0\n",
       "employee_residence          0\n",
       "experience_level            0\n",
       "remote_ratio                0\n",
       "years_experience            0\n",
       "industry                    0\n",
       "posting_date                0\n",
       "application_deadline        0\n",
       "job_description_length      0\n",
       "benefits_score              0\n",
       "company_name                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Explore dataset structure and identify missing values\n",
    "# Purpose:\n",
    "# - Understand column types and non-null counts\n",
    "# - Check for missing values that may impact analysis\n",
    "# - Identify which columns need type conversion (e.g., date fields)\n",
    "\n",
    "df.info()\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72cab93-0250-4c8c-ac95-d97ace54c55d",
   "metadata": {},
   "source": [
    "### 2.2 Data Cleaning â€“ Handling Duplicates\n",
    "\n",
    "To ensure data integrity and avoid biased analysis, we begin the cleaning process by checking for duplicate job listings.\n",
    "\n",
    "#### Duplicate Detection:\n",
    "Using the `df.duplicated()` function, we identified several rows with identical values across all columns. These rows may represent:\n",
    "- Redundant entries due to scraping processes\n",
    "- Re-posted jobs without meaningful changes\n",
    "- Manual entry errors\n",
    "\n",
    "#### Action Taken:\n",
    "We removed the duplicate rows using `df.drop_duplicates()` to preserve only unique job records. This step helps prevent overestimation in frequency-based analysis and improves model generalization in the predictive phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25767633-ee31-442c-a905-a4245b885f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 140\n",
      "New dataset shape after removing duplicates: (15010, 19)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Identify and remove duplicate records\n",
    "# Purpose:\n",
    "# - Prevent bias in frequency counts and model training\n",
    "# - Ensure each job listing is unique\n",
    "\n",
    "# Count duplicates\n",
    "duplicates = df[df.duplicated()]\n",
    "print(f\"Number of duplicate rows: {len(duplicates)}\")\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates()\n",
    "print(f\"New dataset shape after removing duplicates: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c74eee-8358-439e-9262-4e0b1d342a77",
   "metadata": {},
   "source": [
    "After removing the duplicate rows, the dataset now contains **15010 rows**, ensuring that each job listing is unique and representative of the market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4745e9ee-7205-4278-8e4a-a78e97741c33",
   "metadata": {},
   "source": [
    "### 2.3 Data Cleaning â€“ Date Conversion and Temporal Features\n",
    "\n",
    "To enable time-based analysis, we converted `posting_date` and `application_deadline` to proper datetime formats.\n",
    "\n",
    "#### Actions Taken:\n",
    "- Used `pd.to_datetime()` to standardize date formats and handle errors gracefully\n",
    "- Created additional columns for:\n",
    "  - `posting_month`: for monthly trend analysis\n",
    "  - `posting_year`: for year-over-year comparison\n",
    "  - `posting_quarter`: for seasonal/quarterly insights\n",
    "\n",
    "These time features will be instrumental in tracking market evolution and modeling time-dependent trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019ee112-a266-46ee-a367-115e0dfc24f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.3: Convert date fields to datetime format\n",
    "# Purpose:\n",
    "# - Enable temporal analysis (month, quarter, year)\n",
    "# - Calculate job post duration and seasonal trends\n",
    "\n",
    "df['posting_date'] = pd.to_datetime(df['posting_date'], errors='coerce')\n",
    "df['application_deadline'] = pd.to_datetime(df['application_deadline'], errors='coerce')\n",
    "\n",
    "# Create additional time-based features (optional but useful later)\n",
    "df['posting_month'] = df['posting_date'].dt.month\n",
    "df['posting_year'] = df['posting_date'].dt.year\n",
    "df['posting_quarter'] = df['posting_date'].dt.to_period('Q')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21164b29-a081-41cd-b911-24ba9c92ef1a",
   "metadata": {},
   "source": [
    "### 2.4 Data Cleaning â€“ Standardizing Job Titles\n",
    "\n",
    "The `job_title` column contained many inconsistent and misspelled entries, which would affect grouping and aggregation for descriptive and predictive analyses.\n",
    "\n",
    "#### Actions Taken:\n",
    "- Converted all job titles to lowercase and stripped whitespace\n",
    "- Mapped variants and typos (e.g., \"ml ops engineer\", \"nlp specialist\") into standardized job roles using a predefined dictionary\n",
    "- Applied proper casing (e.g., \"AI Software Engineer\") using a smart formatting function\n",
    "\n",
    "This ensures that each job role is uniquely and consistently represented, improving the accuracy of salary imputation and downstream modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb0804c-5bcb-4af4-973e-e234ec8658e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Standardize job titles\n",
    "# Purpose:\n",
    "# - Remove case/typo inconsistency in 'job_title'\n",
    "# - Enable reliable grouping for salary and skill analysis\n",
    "\n",
    "# Define job title mapping for normalization\n",
    "job_mapping = {\n",
    "    'nlp engineer': 'nlp engineer',\n",
    "    'machine learning engineer': 'machine learning engineer',\n",
    "    'robotics engineer': 'robotics engineer',\n",
    "    'data scientist': 'data scientist',\n",
    "    'autonomous systems engineer': 'autonomous systems engineer',\n",
    "    'research scientist': 'research scientist',\n",
    "    'data engineer': 'data engineer',\n",
    "    'deep learning engineer': 'deep learning engineer',\n",
    "    'ai consultant': 'ai consultant',\n",
    "    'ai architect': 'ai architect',\n",
    "    'ai software engineer': 'ai software engineer',\n",
    "    'computer vision engineer': 'computer vision engineer',\n",
    "    'principal data scientist': 'principal data scientist',\n",
    "    'ai product manager': 'ai product manager'\n",
    "    # Add more standard forms if needed\n",
    "}\n",
    "\n",
    "# Lowercase & map\n",
    "df['job_title_lower'] = df['job_title'].str.strip().str.lower()\n",
    "df['job_title_cleaned'] = df['job_title_lower'].map(job_mapping).fillna(df['job_title_lower'])\n",
    "\n",
    "# Smart title formatting\n",
    "def smart_title(job):\n",
    "    job = job.title()\n",
    "    job = job.replace('Ai', 'AI').replace('Nlp', 'NLP').replace('Ml', 'ML')\n",
    "    return job\n",
    "\n",
    "df['job_title_cleaned'] = df['job_title_cleaned'].apply(smart_title)\n",
    "\n",
    "# Finalize column\n",
    "df['job_title'] = df['job_title_cleaned']\n",
    "df.drop(columns=['job_title_lower', 'job_title_cleaned'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e795a-04c5-40aa-b3b2-4e08132a80f6",
   "metadata": {},
   "source": [
    "### 2.5 Handling Missing Values â€“ Salary (salary_usd)\n",
    "\n",
    "The `salary_usd` column had 743 missing values, which could negatively impact both descriptive statistics and model training.\n",
    "\n",
    "#### Imputation Strategy:\n",
    "- We grouped the data by `industry` and `job_title_cleaned` to capture typical salary ranges within each job function and domain.\n",
    "- Then we filled the missing salaries with the **median** of each group.\n",
    "- Median was chosen over mean due to the **right-skewed distribution** of salary (as observed in `df.describe()`), which is typical for compensation data.\n",
    "\n",
    "This method balances contextual accuracy and robustness to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db595761-3d5e-4bdd-aaa2-09387911424a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "          salary_usd  remote_ratio  years_experience  \\\n",
      "count   15009.000000  15010.000000      15010.000000   \n",
      "mean   123826.313945     49.483678          7.196602   \n",
      "min     32519.000000      0.000000          0.000000   \n",
      "25%     72424.000000      0.000000          2.000000   \n",
      "50%    100441.000000     50.000000          5.000000   \n",
      "75%    145785.000000    100.000000         10.000000   \n",
      "max    999999.000000    100.000000        100.000000   \n",
      "std    106309.690532     40.811361         10.887406   \n",
      "\n",
      "                        posting_date           application_deadline  \\\n",
      "count                          15010                          15010   \n",
      "mean   2024-08-29 08:36:48.447701504  2024-10-11 21:33:01.558960896   \n",
      "min              2024-01-01 00:00:00            2024-01-16 00:00:00   \n",
      "25%              2024-04-29 00:00:00            2024-06-13 00:00:00   \n",
      "50%              2024-08-28 00:00:00            2024-10-12 00:00:00   \n",
      "75%              2024-12-29 00:00:00            2025-02-10 00:00:00   \n",
      "max              2025-04-30 00:00:00            2025-07-11 00:00:00   \n",
      "std                              NaN                            NaN   \n",
      "\n",
      "       job_description_length  benefits_score  posting_month  posting_year  \n",
      "count            15010.000000    15010.000000   15010.000000  15010.000000  \n",
      "mean              1503.235710        7.504464       5.513125   2024.244504  \n",
      "min                500.000000        5.000000       1.000000   2024.000000  \n",
      "25%               1003.000000        6.200000       3.000000   2024.000000  \n",
      "50%               1512.000000        7.500000       5.000000   2024.000000  \n",
      "75%               2000.000000        8.800000       8.000000   2024.000000  \n",
      "max               2499.000000       10.000000      12.000000   2025.000000  \n",
      "std                576.083842        1.450991       3.498759      0.429807  \n"
     ]
    }
   ],
   "source": [
    "# Step 2.5: Impute missing values in salary_usd\n",
    "# Strategy:\n",
    "# - Group by industry and standardized job title\n",
    "# - Fill missing salaries with the group median (robust to outliers)\n",
    "\n",
    "df['salary_usd'] = df.groupby(['industry', 'job_title'])['salary_usd']\\\n",
    "                     .transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Confirm all missing values handled\n",
    "print(df['salary_usd'].isnull().sum())  # Expect 0\n",
    "\n",
    "# Optional: Check salary distribution\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe8bf20-a757-4a6c-867a-85fa2fba983c",
   "metadata": {},
   "source": [
    "### 2.6 Handling Missing and Inconsistent Values â€“ Education Requirement\n",
    "\n",
    "The `education_required` column included inconsistent text formatting (e.g., \"phd\", \" PhD \", \"nan\") and 533 missing values.\n",
    "\n",
    "#### Cleaning Steps:\n",
    "- Standardized text to lowercase and stripped whitespace\n",
    "- Converted string `\"nan\"` values to actual `NaN`\n",
    "- Unified capitalization (e.g., \"Phd\" â†’ \"PhD\")\n",
    "\n",
    "#### Imputation Strategy:\n",
    "- Used the **mode of `education_required` per `job_title`** to fill missing values, assuming that education requirements are job-specific.\n",
    "- Applied this logic using `.groupby()` and `.apply()` for row-wise imputation.\n",
    "\n",
    "After these steps, the column is ready for categorical analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c544425-57a5-409f-9a5b-568bf9469791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.6: Clean and impute missing values for 'education_required'\n",
    "# Purpose:\n",
    "# - Standardize inconsistent labels\n",
    "# - Fill missing values based on typical education per job_title\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 1. Normalize and clean formatting\n",
    "df['education_required'] = df['education_required'].str.lower().str.strip()\n",
    "df['education_required'] = df['education_required'].replace('nan', np.nan)\n",
    "\n",
    "# 2. Impute missing values using mode by job_title\n",
    "edu_mode_by_job = df.groupby('job_title')['education_required']\\\n",
    "                    .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "\n",
    "def impute_education(row):\n",
    "    if pd.isna(row['education_required']):\n",
    "        return edu_mode_by_job.get(row['job_title'], 'unknown')\n",
    "    else:\n",
    "        return row['education_required']\n",
    "\n",
    "df['education_required'] = df.apply(impute_education, axis=1)\n",
    "\n",
    "# 3. Capitalize properly: \"phd\" â†’ \"PhD\"\n",
    "df['education_required'] = df['education_required'].str.title()\n",
    "df['education_required'] = df['education_required'].replace({'Phd': 'PhD'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dd9be1-02ba-4a2e-be67-c67d4b4bbc09",
   "metadata": {},
   "source": [
    "### 2.7 Handling Missing Values â€“ Required Skills\n",
    "\n",
    "The `required_skills` column contains key information for job qualification analysis and model input. However, 750 entries were missing.\n",
    "\n",
    "#### Strategy:\n",
    "- **Step 1**: Convert skill strings into lists and standardize formatting\n",
    "- **Step 2**: Group data by `job_title`, `industry`, and `education_required`, and identify the most common 5 skills within each group\n",
    "- **Step 3**: Apply imputation using these skill sets for missing rows\n",
    "- **Step 4**: For rows where the full group is unavailable, apply a fallback method using the most common skills for each `job_title`\n",
    "\n",
    "After processing, all missing values in `required_skills` were successfully filled.\n",
    "\n",
    "> Note: This preserves job-specific relevance while ensuring complete data for downstream analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e681ec55-c2bf-460a-955e-ae3f867ace0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values remaining in 'required_skills': 0\n"
     ]
    }
   ],
   "source": [
    "# Step 2.7: Impute missing values in 'required_skills'\n",
    "# Purpose:\n",
    "# - Fill missing skill sets based on most common combination for similar roles\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Prepare data\n",
    "df_skills = df[df['required_skills'].notna()].copy()\n",
    "df_skills['required_skills_list'] = df_skills['required_skills'].str.strip().str.split(',')\n",
    "\n",
    "# Step 2: Group by relevant dimensions and get most common skill sets\n",
    "group_cols = ['job_title', 'industry', 'education_required']\n",
    "grouped_skills = (\n",
    "    df_skills.groupby(group_cols)['required_skills_list']\n",
    "    .apply(lambda x: [skill.strip() for sublist in x for skill in sublist])\n",
    "    .apply(lambda x: Counter(x).most_common(5))\n",
    ")\n",
    "\n",
    "# Convert to string for replacement\n",
    "grouped_skills = grouped_skills.apply(lambda x: ', '.join([s[0] for s in x]))\n",
    "\n",
    "# Step 3: Fill function\n",
    "def fill_required_skills(row):\n",
    "    if pd.isna(row['required_skills']):\n",
    "        key = (row['job_title'], row['industry'], row['education_required'])\n",
    "        return grouped_skills.get(key, np.nan)\n",
    "    else:\n",
    "        return row['required_skills']\n",
    "\n",
    "df['required_skills_filled'] = df.apply(fill_required_skills, axis=1)\n",
    "\n",
    "# Step 4: Backup method using only job_title\n",
    "# Build fallback skill set by job_title\n",
    "job_title_skills = {}\n",
    "for title, group in df_skills.groupby('job_title'):\n",
    "    all_skills = [skill.strip() for sublist in group['required_skills_list'] for skill in sublist]\n",
    "    most_common = Counter(all_skills).most_common(5)\n",
    "    job_title_skills[title] = ', '.join([s[0] for s in most_common])\n",
    "\n",
    "def fallback_fill(row):\n",
    "    if pd.isna(row['required_skills_filled']):\n",
    "        return job_title_skills.get(row['job_title'], None)\n",
    "    else:\n",
    "        return row['required_skills_filled']\n",
    "\n",
    "df['required_skills'] = df.apply(fallback_fill, axis=1)\n",
    "\n",
    "# Final check\n",
    "print(\"Missing values remaining in 'required_skills':\", df['required_skills'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec780851-c678-4645-891c-286ee440b9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
